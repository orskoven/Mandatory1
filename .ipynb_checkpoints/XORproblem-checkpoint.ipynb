{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2da6da5a-6286-47d7-a897-271764eb2104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# MLPC er godt til læring og hjælper os \"out-of-the-box\" hjælper os meget med opstillingen end andre industry tunge systemer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d0a57a5-79c3-4de8-bc51-6d4338710f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vi opstiller kolonner X indeholder de første to kolonner og sidste ouput er Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea647d87-c05c-4bca-953a-d56b11efb083",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([0, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d655d8ae-e21f-45d7-9a11-fc40cec52e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vi instansiere modellen skal have max_iter over 200 for ikke at gå i stå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed34d7b0-91b7-420a-9b88-7a79ea26bb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpc = MLPClassifier(max_iter=5000, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f93b4263-37aa-41aa-8bf0-d0a32b308b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70141405\n",
      "Iteration 2, loss = 0.69912527\n",
      "Iteration 3, loss = 0.69689697\n",
      "Iteration 4, loss = 0.69472969\n",
      "Iteration 5, loss = 0.69262371\n",
      "Iteration 6, loss = 0.69057895\n",
      "Iteration 7, loss = 0.68855564\n",
      "Iteration 8, loss = 0.68660700\n",
      "Iteration 9, loss = 0.68473921\n",
      "Iteration 10, loss = 0.68292900\n",
      "Iteration 11, loss = 0.68115124\n",
      "Iteration 12, loss = 0.67941821\n",
      "Iteration 13, loss = 0.67773930\n",
      "Iteration 14, loss = 0.67611859\n",
      "Iteration 15, loss = 0.67457587\n",
      "Iteration 16, loss = 0.67303669\n",
      "Iteration 17, loss = 0.67151124\n",
      "Iteration 18, loss = 0.67009949\n",
      "Iteration 19, loss = 0.66869608\n",
      "Iteration 20, loss = 0.66732501\n",
      "Iteration 21, loss = 0.66596608\n",
      "Iteration 22, loss = 0.66461763\n",
      "Iteration 23, loss = 0.66331007\n",
      "Iteration 24, loss = 0.66204325\n",
      "Iteration 25, loss = 0.66082704\n",
      "Iteration 26, loss = 0.65963146\n",
      "Iteration 27, loss = 0.65844792\n",
      "Iteration 28, loss = 0.65725950\n",
      "Iteration 29, loss = 0.65606441\n",
      "Iteration 30, loss = 0.65486302\n",
      "Iteration 31, loss = 0.65366786\n",
      "Iteration 32, loss = 0.65246861\n",
      "Iteration 33, loss = 0.65126411\n",
      "Iteration 34, loss = 0.65005024\n",
      "Iteration 35, loss = 0.64885872\n",
      "Iteration 36, loss = 0.64777046\n",
      "Iteration 37, loss = 0.64665580\n",
      "Iteration 38, loss = 0.64557664\n",
      "Iteration 39, loss = 0.64450228\n",
      "Iteration 40, loss = 0.64339632\n",
      "Iteration 41, loss = 0.64226586\n",
      "Iteration 42, loss = 0.64113837\n",
      "Iteration 43, loss = 0.64003480\n",
      "Iteration 44, loss = 0.63890828\n",
      "Iteration 45, loss = 0.63777160\n",
      "Iteration 46, loss = 0.63661985\n",
      "Iteration 47, loss = 0.63545883\n",
      "Iteration 48, loss = 0.63428130\n",
      "Iteration 49, loss = 0.63318526\n",
      "Iteration 50, loss = 0.63210140\n",
      "Iteration 51, loss = 0.63102209\n",
      "Iteration 52, loss = 0.62992540\n",
      "Iteration 53, loss = 0.62883284\n",
      "Iteration 54, loss = 0.62773233\n",
      "Iteration 55, loss = 0.62661197\n",
      "Iteration 56, loss = 0.62549274\n",
      "Iteration 57, loss = 0.62437681\n",
      "Iteration 58, loss = 0.62329318\n",
      "Iteration 59, loss = 0.62222272\n",
      "Iteration 60, loss = 0.62115357\n",
      "Iteration 61, loss = 0.62006322\n",
      "Iteration 62, loss = 0.61897244\n",
      "Iteration 63, loss = 0.61787673\n",
      "Iteration 64, loss = 0.61674879\n",
      "Iteration 65, loss = 0.61559926\n",
      "Iteration 66, loss = 0.61445319\n",
      "Iteration 67, loss = 0.61330760\n",
      "Iteration 68, loss = 0.61214879\n",
      "Iteration 69, loss = 0.61098346\n",
      "Iteration 70, loss = 0.60982986\n",
      "Iteration 71, loss = 0.60865130\n",
      "Iteration 72, loss = 0.60750560\n",
      "Iteration 73, loss = 0.60634629\n",
      "Iteration 74, loss = 0.60515467\n",
      "Iteration 75, loss = 0.60398096\n",
      "Iteration 76, loss = 0.60280773\n",
      "Iteration 77, loss = 0.60161765\n",
      "Iteration 78, loss = 0.60042908\n",
      "Iteration 79, loss = 0.59920033\n",
      "Iteration 80, loss = 0.59799091\n",
      "Iteration 81, loss = 0.59681062\n",
      "Iteration 82, loss = 0.59561761\n",
      "Iteration 83, loss = 0.59437184\n",
      "Iteration 84, loss = 0.59316202\n",
      "Iteration 85, loss = 0.59194700\n",
      "Iteration 86, loss = 0.59071251\n",
      "Iteration 87, loss = 0.58946712\n",
      "Iteration 88, loss = 0.58821713\n",
      "Iteration 89, loss = 0.58695805\n",
      "Iteration 90, loss = 0.58571269\n",
      "Iteration 91, loss = 0.58445231\n",
      "Iteration 92, loss = 0.58317434\n",
      "Iteration 93, loss = 0.58191851\n",
      "Iteration 94, loss = 0.58068223\n",
      "Iteration 95, loss = 0.57945023\n",
      "Iteration 96, loss = 0.57818467\n",
      "Iteration 97, loss = 0.57688631\n",
      "Iteration 98, loss = 0.57558243\n",
      "Iteration 99, loss = 0.57429604\n",
      "Iteration 100, loss = 0.57299786\n",
      "Iteration 101, loss = 0.57170757\n",
      "Iteration 102, loss = 0.57041391\n",
      "Iteration 103, loss = 0.56911063\n",
      "Iteration 104, loss = 0.56782234\n",
      "Iteration 105, loss = 0.56648919\n",
      "Iteration 106, loss = 0.56516324\n",
      "Iteration 107, loss = 0.56383816\n",
      "Iteration 108, loss = 0.56249030\n",
      "Iteration 109, loss = 0.56117875\n",
      "Iteration 110, loss = 0.55985083\n",
      "Iteration 111, loss = 0.55852248\n",
      "Iteration 112, loss = 0.55711176\n",
      "Iteration 113, loss = 0.55570032\n",
      "Iteration 114, loss = 0.55439928\n",
      "Iteration 115, loss = 0.55306188\n",
      "Iteration 116, loss = 0.55170291\n",
      "Iteration 117, loss = 0.55034013\n",
      "Iteration 118, loss = 0.54897143\n",
      "Iteration 119, loss = 0.54760640\n",
      "Iteration 120, loss = 0.54623881\n",
      "Iteration 121, loss = 0.54483881\n",
      "Iteration 122, loss = 0.54347578\n",
      "Iteration 123, loss = 0.54208530\n",
      "Iteration 124, loss = 0.54070132\n",
      "Iteration 125, loss = 0.53931402\n",
      "Iteration 126, loss = 0.53791862\n",
      "Iteration 127, loss = 0.53650219\n",
      "Iteration 128, loss = 0.53507557\n",
      "Iteration 129, loss = 0.53365129\n",
      "Iteration 130, loss = 0.53223105\n",
      "Iteration 131, loss = 0.53083251\n",
      "Iteration 132, loss = 0.52938699\n",
      "Iteration 133, loss = 0.52799118\n",
      "Iteration 134, loss = 0.52655771\n",
      "Iteration 135, loss = 0.52511242\n",
      "Iteration 136, loss = 0.52367792\n",
      "Iteration 137, loss = 0.52223532\n",
      "Iteration 138, loss = 0.52079972\n",
      "Iteration 139, loss = 0.51935474\n",
      "Iteration 140, loss = 0.51789724\n",
      "Iteration 141, loss = 0.51645188\n",
      "Iteration 142, loss = 0.51496946\n",
      "Iteration 143, loss = 0.51352177\n",
      "Iteration 144, loss = 0.51204632\n",
      "Iteration 145, loss = 0.51055729\n",
      "Iteration 146, loss = 0.50909802\n",
      "Iteration 147, loss = 0.50761109\n",
      "Iteration 148, loss = 0.50613884\n",
      "Iteration 149, loss = 0.50467657\n",
      "Iteration 150, loss = 0.50318023\n",
      "Iteration 151, loss = 0.50171507\n",
      "Iteration 152, loss = 0.50022320\n",
      "Iteration 153, loss = 0.49875235\n",
      "Iteration 154, loss = 0.49724595\n",
      "Iteration 155, loss = 0.49577228\n",
      "Iteration 156, loss = 0.49428815\n",
      "Iteration 157, loss = 0.49274569\n",
      "Iteration 158, loss = 0.49120768\n",
      "Iteration 159, loss = 0.48971983\n",
      "Iteration 160, loss = 0.48822880\n",
      "Iteration 161, loss = 0.48670259\n",
      "Iteration 162, loss = 0.48515351\n",
      "Iteration 163, loss = 0.48364215\n",
      "Iteration 164, loss = 0.48211628\n",
      "Iteration 165, loss = 0.48056755\n",
      "Iteration 166, loss = 0.47904609\n",
      "Iteration 167, loss = 0.47751051\n",
      "Iteration 168, loss = 0.47596044\n",
      "Iteration 169, loss = 0.47442294\n",
      "Iteration 170, loss = 0.47288191\n",
      "Iteration 171, loss = 0.47134271\n",
      "Iteration 172, loss = 0.46979487\n",
      "Iteration 173, loss = 0.46826786\n",
      "Iteration 174, loss = 0.46672842\n",
      "Iteration 175, loss = 0.46518150\n",
      "Iteration 176, loss = 0.46367471\n",
      "Iteration 177, loss = 0.46210737\n",
      "Iteration 178, loss = 0.46052558\n",
      "Iteration 179, loss = 0.45899684\n",
      "Iteration 180, loss = 0.45743786\n",
      "Iteration 181, loss = 0.45585812\n",
      "Iteration 182, loss = 0.45427482\n",
      "Iteration 183, loss = 0.45269623\n",
      "Iteration 184, loss = 0.45114278\n",
      "Iteration 185, loss = 0.44960870\n",
      "Iteration 186, loss = 0.44804642\n",
      "Iteration 187, loss = 0.44645126\n",
      "Iteration 188, loss = 0.44491979\n",
      "Iteration 189, loss = 0.44339718\n",
      "Iteration 190, loss = 0.44178342\n",
      "Iteration 191, loss = 0.44020318\n",
      "Iteration 192, loss = 0.43867924\n",
      "Iteration 193, loss = 0.43712213\n",
      "Iteration 194, loss = 0.43554631\n",
      "Iteration 195, loss = 0.43396633\n",
      "Iteration 196, loss = 0.43240279\n",
      "Iteration 197, loss = 0.43081449\n",
      "Iteration 198, loss = 0.42923551\n",
      "Iteration 199, loss = 0.42765182\n",
      "Iteration 200, loss = 0.42608652\n",
      "Iteration 201, loss = 0.42455745\n",
      "Iteration 202, loss = 0.42300727\n",
      "Iteration 203, loss = 0.42141767\n",
      "Iteration 204, loss = 0.41984206\n",
      "Iteration 205, loss = 0.41828915\n",
      "Iteration 206, loss = 0.41671604\n",
      "Iteration 207, loss = 0.41513867\n",
      "Iteration 208, loss = 0.41355355\n",
      "Iteration 209, loss = 0.41198407\n",
      "Iteration 210, loss = 0.41040733\n",
      "Iteration 211, loss = 0.40885525\n",
      "Iteration 212, loss = 0.40730552\n",
      "Iteration 213, loss = 0.40574083\n",
      "Iteration 214, loss = 0.40413830\n",
      "Iteration 215, loss = 0.40260575\n",
      "Iteration 216, loss = 0.40106067\n",
      "Iteration 217, loss = 0.39948602\n",
      "Iteration 218, loss = 0.39790981\n",
      "Iteration 219, loss = 0.39636707\n",
      "Iteration 220, loss = 0.39481782\n",
      "Iteration 221, loss = 0.39327542\n",
      "Iteration 222, loss = 0.39169000\n",
      "Iteration 223, loss = 0.39014644\n",
      "Iteration 224, loss = 0.38860771\n",
      "Iteration 225, loss = 0.38707958\n",
      "Iteration 226, loss = 0.38551136\n",
      "Iteration 227, loss = 0.38396591\n",
      "Iteration 228, loss = 0.38239088\n",
      "Iteration 229, loss = 0.38085812\n",
      "Iteration 230, loss = 0.37937149\n",
      "Iteration 231, loss = 0.37780580\n",
      "Iteration 232, loss = 0.37627318\n",
      "Iteration 233, loss = 0.37475491\n",
      "Iteration 234, loss = 0.37322607\n",
      "Iteration 235, loss = 0.37170685\n",
      "Iteration 236, loss = 0.37020651\n",
      "Iteration 237, loss = 0.36866124\n",
      "Iteration 238, loss = 0.36719215\n",
      "Iteration 239, loss = 0.36564500\n",
      "Iteration 240, loss = 0.36412284\n",
      "Iteration 241, loss = 0.36262842\n",
      "Iteration 242, loss = 0.36111410\n",
      "Iteration 243, loss = 0.35960618\n",
      "Iteration 244, loss = 0.35811713\n",
      "Iteration 245, loss = 0.35659187\n",
      "Iteration 246, loss = 0.35509242\n",
      "Iteration 247, loss = 0.35359870\n",
      "Iteration 248, loss = 0.35209932\n",
      "Iteration 249, loss = 0.35060712\n",
      "Iteration 250, loss = 0.34911045\n",
      "Iteration 251, loss = 0.34762043\n",
      "Iteration 252, loss = 0.34611529\n",
      "Iteration 253, loss = 0.34462545\n",
      "Iteration 254, loss = 0.34315085\n",
      "Iteration 255, loss = 0.34167915\n",
      "Iteration 256, loss = 0.34020100\n",
      "Iteration 257, loss = 0.33873986\n",
      "Iteration 258, loss = 0.33725819\n",
      "Iteration 259, loss = 0.33577384\n",
      "Iteration 260, loss = 0.33431225\n",
      "Iteration 261, loss = 0.33285829\n",
      "Iteration 262, loss = 0.33142195\n",
      "Iteration 263, loss = 0.32995330\n",
      "Iteration 264, loss = 0.32848266\n",
      "Iteration 265, loss = 0.32702315\n",
      "Iteration 266, loss = 0.32556771\n",
      "Iteration 267, loss = 0.32411995\n",
      "Iteration 268, loss = 0.32264319\n",
      "Iteration 269, loss = 0.32122398\n",
      "Iteration 270, loss = 0.31977179\n",
      "Iteration 271, loss = 0.31831983\n",
      "Iteration 272, loss = 0.31690109\n",
      "Iteration 273, loss = 0.31547965\n",
      "Iteration 274, loss = 0.31404063\n",
      "Iteration 275, loss = 0.31261439\n",
      "Iteration 276, loss = 0.31119425\n",
      "Iteration 277, loss = 0.30980494\n",
      "Iteration 278, loss = 0.30840983\n",
      "Iteration 279, loss = 0.30700924\n",
      "Iteration 280, loss = 0.30559816\n",
      "Iteration 281, loss = 0.30423316\n",
      "Iteration 282, loss = 0.30282618\n",
      "Iteration 283, loss = 0.30139909\n",
      "Iteration 284, loss = 0.29999491\n",
      "Iteration 285, loss = 0.29861725\n",
      "Iteration 286, loss = 0.29722344\n",
      "Iteration 287, loss = 0.29581446\n",
      "Iteration 288, loss = 0.29444531\n",
      "Iteration 289, loss = 0.29307243\n",
      "Iteration 290, loss = 0.29172873\n",
      "Iteration 291, loss = 0.29037767\n",
      "Iteration 292, loss = 0.28896489\n",
      "Iteration 293, loss = 0.28760913\n",
      "Iteration 294, loss = 0.28625750\n",
      "Iteration 295, loss = 0.28489051\n",
      "Iteration 296, loss = 0.28353695\n",
      "Iteration 297, loss = 0.28217842\n",
      "Iteration 298, loss = 0.28081820\n",
      "Iteration 299, loss = 0.27946938\n",
      "Iteration 300, loss = 0.27817758\n",
      "Iteration 301, loss = 0.27685028\n",
      "Iteration 302, loss = 0.27548930\n",
      "Iteration 303, loss = 0.27413978\n",
      "Iteration 304, loss = 0.27285645\n",
      "Iteration 305, loss = 0.27154130\n",
      "Iteration 306, loss = 0.27020031\n",
      "Iteration 307, loss = 0.26889588\n",
      "Iteration 308, loss = 0.26760439\n",
      "Iteration 309, loss = 0.26630824\n",
      "Iteration 310, loss = 0.26502183\n",
      "Iteration 311, loss = 0.26372058\n",
      "Iteration 312, loss = 0.26244211\n",
      "Iteration 313, loss = 0.26117034\n",
      "Iteration 314, loss = 0.25989829\n",
      "Iteration 315, loss = 0.25863150\n",
      "Iteration 316, loss = 0.25737154\n",
      "Iteration 317, loss = 0.25609856\n",
      "Iteration 318, loss = 0.25482722\n",
      "Iteration 319, loss = 0.25357535\n",
      "Iteration 320, loss = 0.25235364\n",
      "Iteration 321, loss = 0.25111897\n",
      "Iteration 322, loss = 0.24988468\n",
      "Iteration 323, loss = 0.24864689\n",
      "Iteration 324, loss = 0.24747283\n",
      "Iteration 325, loss = 0.24622600\n",
      "Iteration 326, loss = 0.24499354\n",
      "Iteration 327, loss = 0.24378490\n",
      "Iteration 328, loss = 0.24257983\n",
      "Iteration 329, loss = 0.24139979\n",
      "Iteration 330, loss = 0.24021856\n",
      "Iteration 331, loss = 0.23903047\n",
      "Iteration 332, loss = 0.23782713\n",
      "Iteration 333, loss = 0.23666389\n",
      "Iteration 334, loss = 0.23547737\n",
      "Iteration 335, loss = 0.23431623\n",
      "Iteration 336, loss = 0.23313309\n",
      "Iteration 337, loss = 0.23196471\n",
      "Iteration 338, loss = 0.23081346\n",
      "Iteration 339, loss = 0.22966158\n",
      "Iteration 340, loss = 0.22849874\n",
      "Iteration 341, loss = 0.22733303\n",
      "Iteration 342, loss = 0.22621202\n",
      "Iteration 343, loss = 0.22509178\n",
      "Iteration 344, loss = 0.22397918\n",
      "Iteration 345, loss = 0.22285029\n",
      "Iteration 346, loss = 0.22173209\n",
      "Iteration 347, loss = 0.22064321\n",
      "Iteration 348, loss = 0.21956095\n",
      "Iteration 349, loss = 0.21846265\n",
      "Iteration 350, loss = 0.21737705\n",
      "Iteration 351, loss = 0.21629200\n",
      "Iteration 352, loss = 0.21520361\n",
      "Iteration 353, loss = 0.21415323\n",
      "Iteration 354, loss = 0.21308124\n",
      "Iteration 355, loss = 0.21199926\n",
      "Iteration 356, loss = 0.21096854\n",
      "Iteration 357, loss = 0.20991399\n",
      "Iteration 358, loss = 0.20885944\n",
      "Iteration 359, loss = 0.20783927\n",
      "Iteration 360, loss = 0.20680187\n",
      "Iteration 361, loss = 0.20576057\n",
      "Iteration 362, loss = 0.20474078\n",
      "Iteration 363, loss = 0.20371223\n",
      "Iteration 364, loss = 0.20267875\n",
      "Iteration 365, loss = 0.20167107\n",
      "Iteration 366, loss = 0.20068403\n",
      "Iteration 367, loss = 0.19966338\n",
      "Iteration 368, loss = 0.19866953\n",
      "Iteration 369, loss = 0.19768070\n",
      "Iteration 370, loss = 0.19671440\n",
      "Iteration 371, loss = 0.19573708\n",
      "Iteration 372, loss = 0.19474401\n",
      "Iteration 373, loss = 0.19377107\n",
      "Iteration 374, loss = 0.19280298\n",
      "Iteration 375, loss = 0.19184575\n",
      "Iteration 376, loss = 0.19088924\n",
      "Iteration 377, loss = 0.18993310\n",
      "Iteration 378, loss = 0.18899840\n",
      "Iteration 379, loss = 0.18803496\n",
      "Iteration 380, loss = 0.18710051\n",
      "Iteration 381, loss = 0.18619020\n",
      "Iteration 382, loss = 0.18526298\n",
      "Iteration 383, loss = 0.18434021\n",
      "Iteration 384, loss = 0.18341905\n",
      "Iteration 385, loss = 0.18250489\n",
      "Iteration 386, loss = 0.18160741\n",
      "Iteration 387, loss = 0.18071149\n",
      "Iteration 388, loss = 0.17980409\n",
      "Iteration 389, loss = 0.17891219\n",
      "Iteration 390, loss = 0.17803773\n",
      "Iteration 391, loss = 0.17716791\n",
      "Iteration 392, loss = 0.17630497\n",
      "Iteration 393, loss = 0.17543083\n",
      "Iteration 394, loss = 0.17454422\n",
      "Iteration 395, loss = 0.17367306\n",
      "Iteration 396, loss = 0.17281701\n",
      "Iteration 397, loss = 0.17198917\n",
      "Iteration 398, loss = 0.17113920\n",
      "Iteration 399, loss = 0.17027278\n",
      "Iteration 400, loss = 0.16940192\n",
      "Iteration 401, loss = 0.16859119\n",
      "Iteration 402, loss = 0.16777335\n",
      "Iteration 403, loss = 0.16693087\n",
      "Iteration 404, loss = 0.16608889\n",
      "Iteration 405, loss = 0.16528775\n",
      "Iteration 406, loss = 0.16447770\n",
      "Iteration 407, loss = 0.16366659\n",
      "Iteration 408, loss = 0.16284617\n",
      "Iteration 409, loss = 0.16204735\n",
      "Iteration 410, loss = 0.16126963\n",
      "Iteration 411, loss = 0.16047990\n",
      "Iteration 412, loss = 0.15968345\n",
      "Iteration 413, loss = 0.15890799\n",
      "Iteration 414, loss = 0.15811665\n",
      "Iteration 415, loss = 0.15734007\n",
      "Iteration 416, loss = 0.15658034\n",
      "Iteration 417, loss = 0.15580102\n",
      "Iteration 418, loss = 0.15503854\n",
      "Iteration 419, loss = 0.15427871\n",
      "Iteration 420, loss = 0.15352590\n",
      "Iteration 421, loss = 0.15277465\n",
      "Iteration 422, loss = 0.15204012\n",
      "Iteration 423, loss = 0.15129979\n",
      "Iteration 424, loss = 0.15057745\n",
      "Iteration 425, loss = 0.14982619\n",
      "Iteration 426, loss = 0.14911095\n",
      "Iteration 427, loss = 0.14838569\n",
      "Iteration 428, loss = 0.14765651\n",
      "Iteration 429, loss = 0.14693865\n",
      "Iteration 430, loss = 0.14624119\n",
      "Iteration 431, loss = 0.14553665\n",
      "Iteration 432, loss = 0.14482092\n",
      "Iteration 433, loss = 0.14411981\n",
      "Iteration 434, loss = 0.14341273\n",
      "Iteration 435, loss = 0.14273357\n",
      "Iteration 436, loss = 0.14205556\n",
      "Iteration 437, loss = 0.14135164\n",
      "Iteration 438, loss = 0.14067372\n",
      "Iteration 439, loss = 0.14000057\n",
      "Iteration 440, loss = 0.13934903\n",
      "Iteration 441, loss = 0.13867313\n",
      "Iteration 442, loss = 0.13799029\n",
      "Iteration 443, loss = 0.13733256\n",
      "Iteration 444, loss = 0.13667528\n",
      "Iteration 445, loss = 0.13601302\n",
      "Iteration 446, loss = 0.13536165\n",
      "Iteration 447, loss = 0.13472786\n",
      "Iteration 448, loss = 0.13409327\n",
      "Iteration 449, loss = 0.13344377\n",
      "Iteration 450, loss = 0.13280136\n",
      "Iteration 451, loss = 0.13216670\n",
      "Iteration 452, loss = 0.13155412\n",
      "Iteration 453, loss = 0.13091865\n",
      "Iteration 454, loss = 0.13029601\n",
      "Iteration 455, loss = 0.12968675\n",
      "Iteration 456, loss = 0.12908069\n",
      "Iteration 457, loss = 0.12847213\n",
      "Iteration 458, loss = 0.12785514\n",
      "Iteration 459, loss = 0.12724054\n",
      "Iteration 460, loss = 0.12666436\n",
      "Iteration 461, loss = 0.12606576\n",
      "Iteration 462, loss = 0.12546179\n",
      "Iteration 463, loss = 0.12489090\n",
      "Iteration 464, loss = 0.12430601\n",
      "Iteration 465, loss = 0.12371340\n",
      "Iteration 466, loss = 0.12313988\n",
      "Iteration 467, loss = 0.12255423\n",
      "Iteration 468, loss = 0.12198651\n",
      "Iteration 469, loss = 0.12141867\n",
      "Iteration 470, loss = 0.12085788\n",
      "Iteration 471, loss = 0.12028762\n",
      "Iteration 472, loss = 0.11971727\n",
      "Iteration 473, loss = 0.11917250\n",
      "Iteration 474, loss = 0.11863408\n",
      "Iteration 475, loss = 0.11807432\n",
      "Iteration 476, loss = 0.11752562\n",
      "Iteration 477, loss = 0.11698189\n",
      "Iteration 478, loss = 0.11643609\n",
      "Iteration 479, loss = 0.11589289\n",
      "Iteration 480, loss = 0.11536132\n",
      "Iteration 481, loss = 0.11482403\n",
      "Iteration 482, loss = 0.11429802\n",
      "Iteration 483, loss = 0.11377700\n",
      "Iteration 484, loss = 0.11325283\n",
      "Iteration 485, loss = 0.11274094\n",
      "Iteration 486, loss = 0.11222765\n",
      "Iteration 487, loss = 0.11170620\n",
      "Iteration 488, loss = 0.11119672\n",
      "Iteration 489, loss = 0.11069115\n",
      "Iteration 490, loss = 0.11018578\n",
      "Iteration 491, loss = 0.10968327\n",
      "Iteration 492, loss = 0.10918701\n",
      "Iteration 493, loss = 0.10869382\n",
      "Iteration 494, loss = 0.10819746\n",
      "Iteration 495, loss = 0.10770616\n",
      "Iteration 496, loss = 0.10721996\n",
      "Iteration 497, loss = 0.10673477\n",
      "Iteration 498, loss = 0.10626007\n",
      "Iteration 499, loss = 0.10577654\n",
      "Iteration 500, loss = 0.10529317\n",
      "Iteration 501, loss = 0.10481008\n",
      "Iteration 502, loss = 0.10434375\n",
      "Iteration 503, loss = 0.10388847\n",
      "Iteration 504, loss = 0.10341431\n",
      "Iteration 505, loss = 0.10294472\n",
      "Iteration 506, loss = 0.10248639\n",
      "Iteration 507, loss = 0.10203489\n",
      "Iteration 508, loss = 0.10157686\n",
      "Iteration 509, loss = 0.10112358\n",
      "Iteration 510, loss = 0.10068390\n",
      "Iteration 511, loss = 0.10022919\n",
      "Iteration 512, loss = 0.09977829\n",
      "Iteration 513, loss = 0.09933858\n",
      "Iteration 514, loss = 0.09889791\n",
      "Iteration 515, loss = 0.09844994\n",
      "Iteration 516, loss = 0.09800821\n",
      "Iteration 517, loss = 0.09755427\n",
      "Iteration 518, loss = 0.09710499\n",
      "Iteration 519, loss = 0.09667456\n",
      "Iteration 520, loss = 0.09623138\n",
      "Iteration 521, loss = 0.09578748\n",
      "Iteration 522, loss = 0.09535285\n",
      "Iteration 523, loss = 0.09491018\n",
      "Iteration 524, loss = 0.09447162\n",
      "Iteration 525, loss = 0.09403829\n",
      "Iteration 526, loss = 0.09361030\n",
      "Iteration 527, loss = 0.09317747\n",
      "Iteration 528, loss = 0.09274410\n",
      "Iteration 529, loss = 0.09231960\n",
      "Iteration 530, loss = 0.09189818\n",
      "Iteration 531, loss = 0.09146484\n",
      "Iteration 532, loss = 0.09104198\n",
      "Iteration 533, loss = 0.09062448\n",
      "Iteration 534, loss = 0.09020993\n",
      "Iteration 535, loss = 0.08980211\n",
      "Iteration 536, loss = 0.08938760\n",
      "Iteration 537, loss = 0.08896166\n",
      "Iteration 538, loss = 0.08855616\n",
      "Iteration 539, loss = 0.08815333\n",
      "Iteration 540, loss = 0.08777177\n",
      "Iteration 541, loss = 0.08738078\n",
      "Iteration 542, loss = 0.08698462\n",
      "Iteration 543, loss = 0.08658553\n",
      "Iteration 544, loss = 0.08619652\n",
      "Iteration 545, loss = 0.08581022\n",
      "Iteration 546, loss = 0.08541774\n",
      "Iteration 547, loss = 0.08502905\n",
      "Iteration 548, loss = 0.08464942\n",
      "Iteration 549, loss = 0.08427183\n",
      "Iteration 550, loss = 0.08389312\n",
      "Iteration 551, loss = 0.08351501\n",
      "Iteration 552, loss = 0.08313694\n",
      "Iteration 553, loss = 0.08275881\n",
      "Iteration 554, loss = 0.08239626\n",
      "Iteration 555, loss = 0.08203291\n",
      "Iteration 556, loss = 0.08164877\n",
      "Iteration 557, loss = 0.08128000\n",
      "Iteration 558, loss = 0.08092201\n",
      "Iteration 559, loss = 0.08055805\n",
      "Iteration 560, loss = 0.08019184\n",
      "Iteration 561, loss = 0.07982943\n",
      "Iteration 562, loss = 0.07947845\n",
      "Iteration 563, loss = 0.07912635\n",
      "Iteration 564, loss = 0.07876592\n",
      "Iteration 565, loss = 0.07841252\n",
      "Iteration 566, loss = 0.07805986\n",
      "Iteration 567, loss = 0.07771166\n",
      "Iteration 568, loss = 0.07737006\n",
      "Iteration 569, loss = 0.07702627\n",
      "Iteration 570, loss = 0.07668394\n",
      "Iteration 571, loss = 0.07633826\n",
      "Iteration 572, loss = 0.07600319\n",
      "Iteration 573, loss = 0.07566492\n",
      "Iteration 574, loss = 0.07532087\n",
      "Iteration 575, loss = 0.07499627\n",
      "Iteration 576, loss = 0.07467108\n",
      "Iteration 577, loss = 0.07433904\n",
      "Iteration 578, loss = 0.07400731\n",
      "Iteration 579, loss = 0.07367362\n",
      "Iteration 580, loss = 0.07334810\n",
      "Iteration 581, loss = 0.07302460\n",
      "Iteration 582, loss = 0.07269599\n",
      "Iteration 583, loss = 0.07238420\n",
      "Iteration 584, loss = 0.07206692\n",
      "Iteration 585, loss = 0.07174101\n",
      "Iteration 586, loss = 0.07142541\n",
      "Iteration 587, loss = 0.07110782\n",
      "Iteration 588, loss = 0.07079125\n",
      "Iteration 589, loss = 0.07047022\n",
      "Iteration 590, loss = 0.07015291\n",
      "Iteration 591, loss = 0.06983901\n",
      "Iteration 592, loss = 0.06952068\n",
      "Iteration 593, loss = 0.06920689\n",
      "Iteration 594, loss = 0.06889849\n",
      "Iteration 595, loss = 0.06859001\n",
      "Iteration 596, loss = 0.06828058\n",
      "Iteration 597, loss = 0.06797248\n",
      "Iteration 598, loss = 0.06766380\n",
      "Iteration 599, loss = 0.06736514\n",
      "Iteration 600, loss = 0.06706646\n",
      "Iteration 601, loss = 0.06676842\n",
      "Iteration 602, loss = 0.06646914\n",
      "Iteration 603, loss = 0.06616773\n",
      "Iteration 604, loss = 0.06586351\n",
      "Iteration 605, loss = 0.06556826\n",
      "Iteration 606, loss = 0.06527944\n",
      "Iteration 607, loss = 0.06498295\n",
      "Iteration 608, loss = 0.06468341\n",
      "Iteration 609, loss = 0.06439320\n",
      "Iteration 610, loss = 0.06411033\n",
      "Iteration 611, loss = 0.06381677\n",
      "Iteration 612, loss = 0.06353418\n",
      "Iteration 613, loss = 0.06325503\n",
      "Iteration 614, loss = 0.06297624\n",
      "Iteration 615, loss = 0.06269583\n",
      "Iteration 616, loss = 0.06241417\n",
      "Iteration 617, loss = 0.06213990\n",
      "Iteration 618, loss = 0.06186526\n",
      "Iteration 619, loss = 0.06158724\n",
      "Iteration 620, loss = 0.06131142\n",
      "Iteration 621, loss = 0.06103829\n",
      "Iteration 622, loss = 0.06077314\n",
      "Iteration 623, loss = 0.06050614\n",
      "Iteration 624, loss = 0.06023756\n",
      "Iteration 625, loss = 0.05996467\n",
      "Iteration 626, loss = 0.05969815\n",
      "Iteration 627, loss = 0.05944116\n",
      "Iteration 628, loss = 0.05917926\n",
      "Iteration 629, loss = 0.05890999\n",
      "Iteration 630, loss = 0.05865059\n",
      "Iteration 631, loss = 0.05839540\n",
      "Iteration 632, loss = 0.05813691\n",
      "Iteration 633, loss = 0.05787923\n",
      "Iteration 634, loss = 0.05761939\n",
      "Iteration 635, loss = 0.05736301\n",
      "Iteration 636, loss = 0.05711868\n",
      "Iteration 637, loss = 0.05686776\n",
      "Iteration 638, loss = 0.05661589\n",
      "Iteration 639, loss = 0.05636466\n",
      "Iteration 640, loss = 0.05611836\n",
      "Iteration 641, loss = 0.05587323\n",
      "Iteration 642, loss = 0.05562689\n",
      "Iteration 643, loss = 0.05537824\n",
      "Iteration 644, loss = 0.05513771\n",
      "Iteration 645, loss = 0.05489817\n",
      "Iteration 646, loss = 0.05465415\n",
      "Iteration 647, loss = 0.05441214\n",
      "Iteration 648, loss = 0.05417725\n",
      "Iteration 649, loss = 0.05393809\n",
      "Iteration 650, loss = 0.05370387\n",
      "Iteration 651, loss = 0.05346940\n",
      "Iteration 652, loss = 0.05323693\n",
      "Iteration 653, loss = 0.05300454\n",
      "Iteration 654, loss = 0.05277027\n",
      "Iteration 655, loss = 0.05253909\n",
      "Iteration 656, loss = 0.05231321\n",
      "Iteration 657, loss = 0.05208894\n",
      "Iteration 658, loss = 0.05186438\n",
      "Iteration 659, loss = 0.05164024\n",
      "Iteration 660, loss = 0.05141357\n",
      "Iteration 661, loss = 0.05119054\n",
      "Iteration 662, loss = 0.05096938\n",
      "Iteration 663, loss = 0.05075250\n",
      "Iteration 664, loss = 0.05053572\n",
      "Iteration 665, loss = 0.05031676\n",
      "Iteration 666, loss = 0.05010121\n",
      "Iteration 667, loss = 0.04989077\n",
      "Iteration 668, loss = 0.04968056\n",
      "Iteration 669, loss = 0.04946464\n",
      "Iteration 670, loss = 0.04925325\n",
      "Iteration 671, loss = 0.04904593\n",
      "Iteration 672, loss = 0.04883565\n",
      "Iteration 673, loss = 0.04863144\n",
      "Iteration 674, loss = 0.04842840\n",
      "Iteration 675, loss = 0.04822220\n",
      "Iteration 676, loss = 0.04801757\n",
      "Iteration 677, loss = 0.04782022\n",
      "Iteration 678, loss = 0.04761825\n",
      "Iteration 679, loss = 0.04742476\n",
      "Iteration 680, loss = 0.04722103\n",
      "Iteration 681, loss = 0.04702734\n",
      "Iteration 682, loss = 0.04682740\n",
      "Iteration 683, loss = 0.04663264\n",
      "Iteration 684, loss = 0.04643992\n",
      "Iteration 685, loss = 0.04624991\n",
      "Iteration 686, loss = 0.04605774\n",
      "Iteration 687, loss = 0.04587088\n",
      "Iteration 688, loss = 0.04567949\n",
      "Iteration 689, loss = 0.04548855\n",
      "Iteration 690, loss = 0.04530212\n",
      "Iteration 691, loss = 0.04512048\n",
      "Iteration 692, loss = 0.04493463\n",
      "Iteration 693, loss = 0.04475185\n",
      "Iteration 694, loss = 0.04456890\n",
      "Iteration 695, loss = 0.04438386\n",
      "Iteration 696, loss = 0.04419894\n",
      "Iteration 697, loss = 0.04402370\n",
      "Iteration 698, loss = 0.04384592\n",
      "Iteration 699, loss = 0.04366292\n",
      "Iteration 700, loss = 0.04349635\n",
      "Iteration 701, loss = 0.04331991\n",
      "Iteration 702, loss = 0.04313963\n",
      "Iteration 703, loss = 0.04296712\n",
      "Iteration 704, loss = 0.04279596\n",
      "Iteration 705, loss = 0.04262590\n",
      "Iteration 706, loss = 0.04245509\n",
      "Iteration 707, loss = 0.04228563\n",
      "Iteration 708, loss = 0.04211792\n",
      "Iteration 709, loss = 0.04195006\n",
      "Iteration 710, loss = 0.04178179\n",
      "Iteration 711, loss = 0.04161627\n",
      "Iteration 712, loss = 0.04145172\n",
      "Iteration 713, loss = 0.04128407\n",
      "Iteration 714, loss = 0.04112127\n",
      "Iteration 715, loss = 0.04095754\n",
      "Iteration 716, loss = 0.04079678\n",
      "Iteration 717, loss = 0.04063513\n",
      "Iteration 718, loss = 0.04047533\n",
      "Iteration 719, loss = 0.04031945\n",
      "Iteration 720, loss = 0.04016149\n",
      "Iteration 721, loss = 0.04000313\n",
      "Iteration 722, loss = 0.03984620\n",
      "Iteration 723, loss = 0.03968773\n",
      "Iteration 724, loss = 0.03953467\n",
      "Iteration 725, loss = 0.03937578\n",
      "Iteration 726, loss = 0.03923083\n",
      "Iteration 727, loss = 0.03908046\n",
      "Iteration 728, loss = 0.03892449\n",
      "Iteration 729, loss = 0.03877657\n",
      "Iteration 730, loss = 0.03862850\n",
      "Iteration 731, loss = 0.03847717\n",
      "Iteration 732, loss = 0.03832750\n",
      "Iteration 733, loss = 0.03818132\n",
      "Iteration 734, loss = 0.03803407\n",
      "Iteration 735, loss = 0.03789245\n",
      "Iteration 736, loss = 0.03775003\n",
      "Iteration 737, loss = 0.03760276\n",
      "Iteration 738, loss = 0.03746157\n",
      "Iteration 739, loss = 0.03732092\n",
      "Iteration 740, loss = 0.03717934\n",
      "Iteration 741, loss = 0.03703734\n",
      "Iteration 742, loss = 0.03689797\n",
      "Iteration 743, loss = 0.03675740\n",
      "Iteration 744, loss = 0.03661773\n",
      "Iteration 745, loss = 0.03648235\n",
      "Iteration 746, loss = 0.03634768\n",
      "Iteration 747, loss = 0.03620910\n",
      "Iteration 748, loss = 0.03607569\n",
      "Iteration 749, loss = 0.03593990\n",
      "Iteration 750, loss = 0.03580470\n",
      "Iteration 751, loss = 0.03567053\n",
      "Iteration 752, loss = 0.03553940\n",
      "Iteration 753, loss = 0.03540927\n",
      "Iteration 754, loss = 0.03527868\n",
      "Iteration 755, loss = 0.03514457\n",
      "Iteration 756, loss = 0.03501692\n",
      "Iteration 757, loss = 0.03488626\n",
      "Iteration 758, loss = 0.03476024\n",
      "Iteration 759, loss = 0.03463202\n",
      "Iteration 760, loss = 0.03450568\n",
      "Iteration 761, loss = 0.03437927\n",
      "Iteration 762, loss = 0.03425296\n",
      "Iteration 763, loss = 0.03412947\n",
      "Iteration 764, loss = 0.03400453\n",
      "Iteration 765, loss = 0.03388057\n",
      "Iteration 766, loss = 0.03375809\n",
      "Iteration 767, loss = 0.03363544\n",
      "Iteration 768, loss = 0.03351464\n",
      "Iteration 769, loss = 0.03339291\n",
      "Iteration 770, loss = 0.03327375\n",
      "Iteration 771, loss = 0.03315230\n",
      "Iteration 772, loss = 0.03303223\n",
      "Iteration 773, loss = 0.03291559\n",
      "Iteration 774, loss = 0.03279711\n",
      "Iteration 775, loss = 0.03268007\n",
      "Iteration 776, loss = 0.03256147\n",
      "Iteration 777, loss = 0.03244577\n",
      "Iteration 778, loss = 0.03233053\n",
      "Iteration 779, loss = 0.03221631\n",
      "Iteration 780, loss = 0.03210249\n",
      "Iteration 781, loss = 0.03198596\n",
      "Iteration 782, loss = 0.03187687\n",
      "Iteration 783, loss = 0.03176581\n",
      "Iteration 784, loss = 0.03165504\n",
      "Iteration 785, loss = 0.03154114\n",
      "Iteration 786, loss = 0.03143134\n",
      "Iteration 787, loss = 0.03132266\n",
      "Iteration 788, loss = 0.03121171\n",
      "Iteration 789, loss = 0.03110411\n",
      "Iteration 790, loss = 0.03099262\n",
      "Iteration 791, loss = 0.03088733\n",
      "Iteration 792, loss = 0.03078448\n",
      "Iteration 793, loss = 0.03067797\n",
      "Iteration 794, loss = 0.03056802\n",
      "Iteration 795, loss = 0.03046222\n",
      "Iteration 796, loss = 0.03035746\n",
      "Iteration 797, loss = 0.03025478\n",
      "Iteration 798, loss = 0.03015041\n",
      "Iteration 799, loss = 0.03004473\n",
      "Iteration 800, loss = 0.02993988\n",
      "Iteration 801, loss = 0.02984063\n",
      "Iteration 802, loss = 0.02973841\n",
      "Iteration 803, loss = 0.02963577\n",
      "Iteration 804, loss = 0.02953443\n",
      "Iteration 805, loss = 0.02943884\n",
      "Iteration 806, loss = 0.02933439\n",
      "Iteration 807, loss = 0.02923557\n",
      "Iteration 808, loss = 0.02913805\n",
      "Iteration 809, loss = 0.02904037\n",
      "Iteration 810, loss = 0.02894262\n",
      "Iteration 811, loss = 0.02884519\n",
      "Iteration 812, loss = 0.02874715\n",
      "Iteration 813, loss = 0.02865356\n",
      "Iteration 814, loss = 0.02855712\n",
      "Iteration 815, loss = 0.02846302\n",
      "Iteration 816, loss = 0.02836841\n",
      "Iteration 817, loss = 0.02827243\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-3 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-3 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-3 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-3 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-3 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(max_iter=5000, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;MLPClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(max_iter=5000, verbose=True)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(max_iter=5000, verbose=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlpc.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdf8993e-0470-4ea0-bbe8-127789321092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlpc.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55b518f8-7440-40fe-9779-55616ea56cce",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib' has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m(X)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf-metal/lib/python3.9/site-packages/matplotlib/_api/__init__.py:217\u001b[0m, in \u001b[0;36mcaching_module_getattr.<locals>.__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m props:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m props[name]\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance)\n\u001b[0;32m--> 217\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'matplotlib' has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "plt.show(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82d36332-4137-41ce-b6cd-4e010bf232e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual errors  -> accuracy er det dårligt målenhed\n",
    "from IPython.display import display, Math, Markdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c67e571-80df-465a-b191-b4a88a634658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "###  Adam optimizer cost functions"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "Vi er interessert i at træne til at cost kurven flader ud \n",
       "        \n",
       "        - > Vi vil gøre tingene så effektivt som muligt og undgå overfitting\n",
       "\n",
       "**Cost modellen for regression**\n",
       "\n",
       "        - > MEAN SQUARED ERROR \n",
       "\n",
       "        - > \n",
       "\n",
       "**Binary Classification**\n",
       "\n",
       "        - > Y i ligningen gør at alt ganget med nul forsvinder\n",
       "        - > enten log X eller log 1-h(x)\n",
       "        - > halvdelen af ligningen forsvinder altid \n",
       "\n",
       "\n",
       "\n",
       "**Prevent exploding eller vanishing gradient**\n",
       "\n",
       "        - > Regularization kan modarbajde og være løsningen\n",
       "        - > Ligger learning params til costen bliver en omkostning for modellen -> noget givet ved learning params til kosten\n",
       "        - > tvinger modellen at holde learning params i nærheden af nul og får modellerne til at performe bedre\n",
       "        - > Specielt ved anvendelse af ReLU \n",
       "        - > ligger regularization til kost tvinger learning params omkring nul\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "**Performance Measure for Regression Models**\n",
       "\n",
       "        - > Mean square error\n",
       "        - > OR\n",
       "        - > root mean square error\n",
       "        - > OR\n",
       "        - > mean absolute error\n",
       "\n",
       "\n",
       "**Performance Measures for Classification**\n",
       "\n",
       "        - > Yoden: Udfordring i medicin rask : lable 1 og syge label: 0 (god model kan være bare svare 1 og få god accuracy)\n",
       "        - > bare sige alle er raske\n",
       "        - > komme til den forkerte konklusion udelukkende fra accuracy -> LÆS ARTIKEL\n",
       "        - > youden index giver h og y ( h er matrix for prediction og y er ground\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cost function\n",
    "display(Markdown(\"###  Adam optimizer cost functions\"))\n",
    "display(Markdown(\"\"\"\n",
    "Vi er interessert i at træne til at cost kurven flader ud \n",
    "        \n",
    "        - > Vi vil gøre tingene så effektivt som muligt og undgå overfitting\n",
    "\n",
    "**Cost modellen for regression**\n",
    "\n",
    "        - > MEAN SQUARED ERROR \n",
    "\n",
    "        - > \n",
    "\n",
    "**Binary Classification**\n",
    "\n",
    "        - > Y i ligningen gør at alt ganget med nul forsvinder\n",
    "        - > enten log X eller log 1-h(x)\n",
    "        - > halvdelen af ligningen forsvinder altid \n",
    "\n",
    "\n",
    "\n",
    "**Prevent exploding eller vanishing gradient**\n",
    "\n",
    "        - > Regularization kan modarbajde og være løsningen\n",
    "        - > Ligger learning params til costen bliver en omkostning for modellen -> noget givet ved learning params til kosten\n",
    "        - > tvinger modellen at holde learning params i nærheden af nul og får modellerne til at performe bedre\n",
    "        - > Specielt ved anvendelse af ReLU \n",
    "        - > ligger regularization til kost tvinger learning params omkring nul\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Performance Measure for Regression Models**\n",
    "\n",
    "        - > Mean square error\n",
    "        - > OR\n",
    "        - > root mean square error\n",
    "        - > OR\n",
    "        - > mean absolute error\n",
    "\n",
    "\n",
    "**Performance Measures for Classification**\n",
    "\n",
    "        - > Yoden: Udfordring i medicin rask : lable 1 og syge label: 0 (god model kan være bare svare 1 og få god accuracy)\n",
    "        - > bare sige alle er raske\n",
    "        - > komme til den forkerte konklusion udelukkende fra accuracy -> LÆS ARTIKEL\n",
    "        - > youden index giver h og y ( h er matrix for prediction og y er ground\n",
    "\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ffeeab06-9963-4e45-971a-1ee0c3f350f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(H, Y, beta=1.0):\n",
    "   tp = sum((Y == H) * (Y == 1) * 1)\n",
    "   tn = sum((Y == H) * (Y == 0) * 1)\n",
    "   fp = sum((Y != H) * (Y == 0) * 1)\n",
    "   fn = sum((Y != H) * (Y == 1) * 1)\n",
    "\n",
    "   accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "   sensitivity = tp / (tp + fn)\n",
    "   specificity = tn / (fp + tn)\n",
    "   precision = tp / (tp + fp)\n",
    "   recall = sensitivity\n",
    "   f_score = ( (beta**2 + 1) * precision * recall) / (beta**2 * precision + recall)\n",
    "   auc = (sensitivity + specificity) / 2\n",
    "   youden = sensitivity - (1 - specificity)\n",
    "   p_plus = sensitivity / (1 - specificity)\n",
    "   p_minus = (1 - sensitivity) / specificity\n",
    "   dp = (np.sqrt(3) / np.pi) * (np.log(sensitivity/(1 - sensitivity) + np.log(specificity/(1 - specificity))))\n",
    "\n",
    "   result = {}\n",
    "   result[\"tp\"] = tp\n",
    "   result[\"tn\"] = tn\n",
    "   result[\"fp\"] = fp\n",
    "   result[\"fn\"] = fn\n",
    "   result[\"accuracy\"] = accuracy\n",
    "   result[\"sensitivity\"] = sensitivity\n",
    "   result[\"specificity\"] = specificity\n",
    "   result[\"precision\"] = precision\n",
    "   result[\"recall\"] = recall\n",
    "   result[\"f-score\"] = f_score\n",
    "   result[\"AUC\"] = auc\n",
    "   result[\"Youden\"] = youden\n",
    "   result[\"p+\"] = p_plus\n",
    "   result[\"p-\"] = p_minus\n",
    "   result[\"DP\"] = dp\n",
    "\n",
    "   return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fdee6fdd-ad3a-467a-9f38-6a42409b252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as random\n",
    "\n",
    "m  = 10_000_000\n",
    "p  = 150\n",
    "fp = 120\n",
    "\n",
    "y = np.zeros(m)\n",
    "h = np.zeros(m)\n",
    "\n",
    "# inject positives and 20% true positives, 80% false negatives\n",
    "for i in range(p):\n",
    "   idx = random.randint(0, m - 1)\n",
    "   y[idx] = 1\n",
    "   if random.random() > 0.8: h[idx] = 1\n",
    "\n",
    "# inject false positive\n",
    "for i in range(fp):\n",
    "   idx = random.randint(0, m - 1)\n",
    "   h[idx] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ca11e662-7475-4412-b22c-f26250e25026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "\n",
       "\n",
       "**Performance Measures for Classification**\n",
       "\n",
       "        - > Yoden: Udfordring i medicin rask : lable 1 og syge label: 0 (god model kan være bare svare 1 og få god accuracy)\n",
       "        - > bare sige alle er raske\n",
       "        - > komme til den forkerte konklusion udelukkende fra accuracy -> LÆS ARTIKEL\n",
       "        - > youden index giver h og y ( h er matrix for prediction og y er ground\n",
       "\n",
       "**Youdon index super bedre til accucracy of binary classification**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cost function\n",
    "display(Markdown(\"\"))\n",
    "display(Markdown(\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "**Performance Measures for Classification**\n",
    "\n",
    "        - > Yoden: Udfordring i medicin rask : lable 1 og syge label: 0 (god model kan være bare svare 1 og få god accuracy)\n",
    "        - > bare sige alle er raske\n",
    "        - > komme til den forkerte konklusion udelukkende fra accuracy -> LÆS ARTIKEL\n",
    "        - > youden index giver h og y ( h er matrix for prediction og y er ground\n",
    "\n",
    "**Youdon index super bedre til accucracy of binary classification**\n",
    "\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9d3336d6-693a-48e8-8948-8b9ee8cca312",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 10_000_000\n",
    "y = np.random.randint(2, size=m)\n",
    "h = np.random.randint(2, size=m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "61e7fd5d-f0c2-4389-aa7e-942b53fb6f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "\n",
       "\n",
       "**MNIST - > god til læring**\n",
       "\n",
       "        - > BITMAP: 28 * 28 * 70.000 = data set 255 gråtoner \n",
       "\n",
       "\n",
       "\n",
       "**Execution plan**\n",
       "\n",
       "    1. Fetch mnist dataset \n",
       "    2. Sanity check: mnist.data.shape \n",
       "    \n",
       "**Preprocess**\n",
       "\n",
       "4. Split, seperate to \n",
       "   \n",
       "    \n",
       "    X:\n",
       "       \n",
       "        \n",
       "        -> training  -> training\n",
       "        -> validation  -> sammenligner med det samme i træning\n",
       "        -> test  -> sidste objektive måle punkt -> sættes til side og skal først bruges helt til sidst ikke bruge før jeg ikke ikke rør modellen mere\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cost function\n",
    "display(Markdown(\"\"))\n",
    "display(Markdown(\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "**MNIST - > god til læring**\n",
    "\n",
    "        - > BITMAP: 28 * 28 * 70.000 = data set 255 gråtoner \n",
    "\n",
    "\n",
    "\n",
    "**Execution plan**\n",
    "\n",
    "    1. Fetch mnist dataset \n",
    "    2. Sanity check: mnist.data.shape \n",
    "    \n",
    "**Preprocess**\n",
    "\n",
    "4. Split, seperate to \n",
    "   \n",
    "    \n",
    "    X:\n",
    "       \n",
    "        \n",
    "        -> training  -> training\n",
    "        -> validation  -> sammenligner med det samme i træning\n",
    "        -> test  -> sidste objektive måle punkt -> sættes til side og skal først bruges helt til sidst ikke bruge før jeg ikke ikke rør modellen mere\n",
    "\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0e050439-8327-4755-96ab-9a7334c22214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "db00e06d-5c14-4085-88d0-9b627d05222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784', data_home='~/datasets/mnist')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7a058c15-9b99-4ef1-b62b-64f479f16fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check dataset: 70,000 samples, 784 features\n",
    "mnist.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "99d6d1b4-5fba-4f71-ba07-74eee281bf74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['5', '0', '4', ..., '4', '5', '6'], dtype=object)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z = np.c_[mnist.target, mnist.data]\n",
    "Z[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "49e965e4-5aeb-4753-9855-7300f4b87226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data and labels into X and Y\n",
    "X = Z[:,1:]\n",
    "Y = Z[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "17244290-3c3b-4a46-9ffc-5d2dd79cf1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 1030\n",
    "X = np.asarray(X, dtype=int)\n",
    "print(Y[idx])\n",
    "img = plt.imshow(X[idx].reshape(28,28), cmap='gray_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "33bd05bd-8d55-4acc-96e5-eecdeaff7109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set\n",
    "X_train = X[0:50000]\n",
    "Y_train = Y[0:50000]\n",
    "\n",
    "# validation set\n",
    "X_val = X[50000:60000]\n",
    "Y_val = Y[50000:60000]\n",
    "\n",
    "# test set\n",
    "X_test = X[60000:70000]\n",
    "Y_test = Y[60000:70000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6c740113-38b0-4d48-b37d-58f5628ec089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cde94595-ef7a-4dad-9470-60be631f8173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Liste af modeller:** "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "display(Markdown(\"\"\"**Liste af modeller:** \"\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2433ee20-859e-4b46-bd76-1ffe78f8e0c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b045e83f-346a-495e-886f-b8f7b28fd215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b5a10ef2-fb23-445c-9e99-f399c1081541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Implementering af modellerne:** "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier_names = [\n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\", \n",
    "    \"Neural Net (75, 75)\", \n",
    "    \"Neural Net (784, 784, 784)\", \n",
    "    \"Naive Bayes\"\n",
    "]\n",
    "\n",
    "display(Markdown(\"\"\"**Implementering af modellerne:** \"\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f8f1b397-8668-4ca9-99f0-b88fa5e80097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Implementering af modellerne:** "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifiers = [\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(n_estimators=100),\n",
    "    MLPClassifier(hidden_layer_sizes=(75, 75)),\n",
    "    MLPClassifier(hidden_layer_sizes=(784, 784, 784)),\n",
    "    GaussianNB(),\n",
    "]\n",
    "\n",
    "display(Markdown(\"\"\"**Implementering af modellerne:** \"\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "39421f21-16b1-4a4a-b556-6a39a60d63bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Decision Tree\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m** \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclf_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 4\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mTraining time:\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mt1\u001b[38;5;241m-\u001b[39mt0\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m3.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf-metal/lib/python3.9/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf-metal/lib/python3.9/site-packages/sklearn/tree/_classes.py:1009\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    979\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    980\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \n\u001b[1;32m    982\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1009\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1015\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf-metal/lib/python3.9/site-packages/sklearn/tree/_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    463\u001b[0m         splitter,\n\u001b[1;32m    464\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    470\u001b[0m     )\n\u001b[0;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for clf, clf_name in zip(classifiers, classifier_names):\n",
    "    print(f\"** {clf_name}\")\n",
    "    t0 = time.time()\n",
    "    clf.fit(X_train, Y_train)\n",
    "    t1 = time.time()\n",
    "    print(f\"\\tTraining time:\\t\\t{t1-t0:3.3f}\")\n",
    "    score_train = clf.score(X_train[0:10000], Y_train[0:10000])\n",
    "    t2 = time.time()\n",
    "    print(f\"\\tPrediction time(train):\\t{t2-t1:3.3f}\")\n",
    "    score_test = clf.score(X_test, Y_test)\n",
    "    t3 = time.time()\n",
    "    print(f\"\\tPrediction time(test):\\t{t3-t2:3.3f}\")\n",
    "    print(f\"\\tScore Train: {score_train:.3f}\\tScore Test: {score_test:.3f}\")\n",
    "\n",
    "display(Markdown(\"\"\"**Træning og metrics:** \"\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e0dff87b-c3d6-4574-992c-eb86ea8bff7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.966 108.39876198768616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonbeckmann/miniforge3/envs/tf-metal/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:697: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8926 3.2498550415039062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonbeckmann/miniforge3/envs/tf-metal/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:697: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8745 2.877995014190674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonbeckmann/miniforge3/envs/tf-metal/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:697: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4135 0.21998381614685059\n",
      "0.8427 0.7596871852874756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonbeckmann/miniforge3/envs/tf-metal/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:697: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "# default alpha=0.0001\n",
    "for a in [0.0001, 0.001, 0.01, 0.1, 1]:\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(75, 75), alpha=a)\n",
    "    t0 = time.time()\n",
    "    mlp.fit(X_train, Y_train)\n",
    "    t1 = time.time()\n",
    "    print(mlp.score(X_test, Y_test), t1 - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2676f745-65f0-4d10-a231-96417ca10526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69e33a1-0a59-491d-9040-2e8ff1f79c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cdc369b7-09ee-4530-94de-8ea13f1d736a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonbeckmann/miniforge3/envs/tf-metal/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:697: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2268 0.39035725593566895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonbeckmann/miniforge3/envs/tf-metal/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:697: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.219 0.1817479133605957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonbeckmann/miniforge3/envs/tf-metal/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:697: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4882 0.40285825729370117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonbeckmann/miniforge3/envs/tf-metal/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:697: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0687 0.1961510181427002\n",
      "0.0648 0.1663978099822998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonbeckmann/miniforge3/envs/tf-metal/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:697: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "for hl in [(25), (50), (50, 50), (100), (100, 100)]:\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=hl)\n",
    "    t0 = time.time()\n",
    "    mlp.fit(X_train, Y_train)\n",
    "    t1 = time.time()\n",
    "    print(mlp.score(X_test, Y_test), t1 - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1e0082e8-cc5a-4151-98bc-ae37e64753c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Beslutning af antal noder i hvertlag?** \n",
       "\n",
       "\n",
       "1. Prøve sig frem\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"\"\"**Beslutning af antal noder i hvertlag?** \n",
    "\n",
    "\n",
    "1. Prøve sig frem\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8df68d8-8838-4a9f-aee9-77e859a3d9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow Metal",
   "language": "python",
   "name": "tf-metal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
